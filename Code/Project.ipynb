{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johnbiggan/DeepLearningforHealthcare-Project/blob/main/Code/Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQ0sNuMePBXx"
      },
      "source": [
        "# Introduction\n",
        "This is an introduction to your report, you should edit this text/mardown section to compose. In this text/markdown, you should introduce:\n",
        "\n",
        "*   Background of the problem\n",
        "  * what type of problem: disease/readmission/mortality prediction,  feature engineeing, data processing, etc\n",
        "  * what is the importance/meaning of solving the problem\n",
        "  * what is the difficulty of the problem\n",
        "  * the state of the art methods and effectiveness.\n",
        "*   Paper explanation\n",
        "  * what did the paper propose\n",
        "  * what is the innovations of the method\n",
        "  * how well the proposed method work (in its own metrics)\n",
        "  * what is the contribution to the reasearch regime (referring the Background above, how important the paper is to the problem).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABD4VhFZbehA"
      },
      "outputs": [],
      "source": [
        "# code comment is used as inline annotations for your coding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uygL9tTPSVHB"
      },
      "source": [
        "# Scope of Reproducibility:\n",
        "\n",
        "Hypothesis 1: Including the time between visits will improve the model over the base model that treats all visits in a simple sequential manner.\n",
        "\n",
        "Hypothesis 2: Including domain knowledge, along with the time between visits, will lead to a better performing model than the base model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      },
      "source": [
        "# Methodology\n",
        "\n",
        "This methodology is the core of your project. It consists of run-able codes with necessary annotations to show the expeiment you executed for testing the hypotheses.\n",
        "\n",
        "The methodology at least contains two subsections **data** and **model** in your experiment."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pyhealth"
      ],
      "metadata": {
        "id": "y5Cr-eC9_TLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "gT-JlpwsCKhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NbPHUTMbkD3"
      },
      "source": [
        "##  Data\n",
        "Data includes raw data (MIMIC III tables), descriptive statistics (our homework questions), and data processing (feature engineering).\n",
        "  * Source of the data: where the data is collected from; if data is synthetic or self-generated, explain how. If possible, please provide a link to the raw datasets.\n",
        "  * Statistics: include basic descriptive statistics of the dataset like size, cross validation split, label distribution, etc.\n",
        "  * Data process: how do you munipulate the data, e.g., change the class labels, split the dataset to train/valid/test, refining the dataset.\n",
        "  * Illustration: printing results, plotting figures for illustration.\n",
        "  * You can upload your raw dataset to Google Drive and mount this Colab to the same directory. If your raw dataset is too large, you can upload the processed dataset and have a code to load the processed dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzVUQS0CHry0"
      },
      "outputs": [],
      "source": [
        "from pyhealth.datasets import MIMIC3Dataset\n",
        "from pyhealth.datasets import split_by_patient, get_dataloader\n",
        "from pyhealth.models import RNN\n",
        "from pyhealth.trainer import Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aYF3qCv_HvF"
      },
      "outputs": [],
      "source": [
        "base_dataset = MIMIC3Dataset(\n",
        "    root=\"https://storage.googleapis.com/pyhealth/Synthetic_MIMIC-III/\",\n",
        "    tables=[\"DIAGNOSES_ICD\", \"PROCEDURES_ICD\"],\n",
        "    code_mapping={\"ICD9CM\": \"CCSCM\", \"ICD9PROC\": \"CCSPROC\"},\n",
        "    dev=False,\n",
        "    refresh_cache=False,\n",
        ")\n",
        "base_dataset.stat()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsLJXUBQ_HvF"
      },
      "outputs": [],
      "source": [
        "from pyhealth.medcode import CrossMap\n",
        "from pyhealth.medcode import InnerMap\n",
        "\n",
        "mapping = CrossMap.load(source_vocabulary=\"ICD9CM\", target_vocabulary=\"CCSCM\")\n",
        "print(\"The CCSCM code that maps to ICD-9 code 428.9 (conjestive heart failure) is\", mapping.map(\"428.9\"))\n",
        "\n",
        "ccscm = InnerMap.load(\"CCSCM\")\n",
        "print(\"Confirmation that CCSCM code '108' corresponds to\", ccscm.lookup(\"108\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWOyusf9_HvF"
      },
      "outputs": [],
      "source": [
        "# Create custom visit time difference calculation and heart failure prediction task\n",
        "from pyhealth.data import Patient, Visit\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def visit_time_diff_mimic3_fn(patient: Patient):\n",
        "    \"\"\"Processes a single patient for the visit time difference task.\n",
        "\n",
        "    Visit time difference calculates the delay between the current visit and\n",
        "    the previous visit.\n",
        "\n",
        "    Args:\n",
        "        patient: a Patient object\n",
        "\n",
        "    Returns:\n",
        "        samples: a list of samples, each sample is a dict with patient_id,\n",
        "            visit_id, and other task-specific attributes as key\n",
        "\n",
        "    Examples:\n",
        "        >>> from pyhealth.datasets import MIMIC3Dataset\n",
        "        >>> mimic3_base = MIMIC3Dataset(\n",
        "        ...    root=\"/srv/local/data/physionet.org/files/mimiciii/1.4\",\n",
        "        ...    tables=[\"DIAGNOSES_ICD\", \"PROCEDURES_ICD\", \"PRESCRIPTIONS\"],\n",
        "        ...    code_mapping={\"ICD9CM\": \"CCSCM\"},\n",
        "        ... )\n",
        "        >>> from pyhealth.tasks import hf_prediction_mimic3_fn\n",
        "        >>> mimic3_sample = mimic3_base.set_task(visit_time_diff_mimic3_fn)\n",
        "        >>> mimic3_sample.samples[0]\n",
        "        [{'visit_id': '130744', 'patient_id': '103', 'conditions': [['42', '109', '19', '122', '98', '663', '58', '51']], 'procedures': [['1']], 'visit_diff': [[0.0, 0.0, 0.0, 0.0]] 'label': 0}]\n",
        "    \"\"\"\n",
        "    samples = []\n",
        "    criterion_time = None\n",
        "\n",
        "    for i in range(len(patient) - 1):\n",
        "        visit: Visit = patient[i]\n",
        "        visit_time = visit.encounter_time\n",
        "\n",
        "        next_visit: Visit = patient[i + 1]\n",
        "        hf_label = 0\n",
        "\n",
        "        if '108' not in next_visit.get_code_list(table=\"DIAGNOSES_ICD\"):\n",
        "            hf_label = 0\n",
        "        else:\n",
        "            hf_label = 1\n",
        "\n",
        "        visit_diff = []\n",
        "\n",
        "        if i > 0:\n",
        "            prev_visit: Visit = patient[i - 1]\n",
        "            prev_visit_time = prev_visit.encounter_time\n",
        "\n",
        "            # Find criterion time (i.e. first encounter with HF diagnosis)\n",
        "            for j in range(len(patient) - 1):\n",
        "                c_visit: Visit = patient[j]\n",
        "                if criterion_time == None and '108' in c_visit.get_code_list(table=\"DIAGNOSES_ICD\"):\n",
        "                    criterion_time = c_visit.encounter_time\n",
        "\n",
        "            if criterion_time != None:\n",
        "                v_c_s = np.sin(((visit_time - criterion_time).days)/10000).item()\n",
        "                v_c_c = np.cos(((visit_time - criterion_time).days)/10000).item()\n",
        "            else:\n",
        "                v_c = 0.0\n",
        "                v_c_s = 0.0\n",
        "                v_c_c = 0.0\n",
        "\n",
        "            v_p = visit_time - prev_visit_time\n",
        "\n",
        "            visit_diff = [\n",
        "                v_c_s,\n",
        "                v_c_c,\n",
        "                np.sin(((v_p).days)/10000).item(),\n",
        "                np.cos(((v_p).days)/10000).item()\n",
        "            ]\n",
        "\n",
        "        else:\n",
        "            visit_diff = [0.0, 0.0, 0.0, 0.0]\n",
        "\n",
        "        conditions = visit.get_code_list(table=\"DIAGNOSES_ICD\")\n",
        "        procedures = visit.get_code_list(table=\"PROCEDURES_ICD\")\n",
        "        # exclude: visits without condition and procedure code\n",
        "        if len(conditions) * len(procedures) == 0 or len(patient) < 2:\n",
        "            continue\n",
        "        samples.append(\n",
        "            {\n",
        "                \"visit_id\": visit.visit_id,\n",
        "                \"patient_id\": patient.patient_id,\n",
        "                \"conditions\": [conditions],\n",
        "                \"procedures\": [procedures],\n",
        "                \"visit_diff\": [visit_diff],\n",
        "                \"label\": hf_label,\n",
        "            }\n",
        "        )\n",
        "    # no cohort selection\n",
        "    return samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFsqr4s6_HvG"
      },
      "outputs": [],
      "source": [
        "sample_dataset = base_dataset.set_task(visit_time_diff_mimic3_fn)\n",
        "sample_dataset.stat()\n",
        "print(sample_dataset[9]) #9\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = split_by_patient(\n",
        "    sample_dataset, [0.8, 0.1, 0.1]\n",
        ")\n",
        "\n",
        "train_dataloader = get_dataloader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_dataloader = get_dataloader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_dataloader = get_dataloader(test_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiHVzIcG_HvG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the CSV data into a pandas DataFrame\n",
        "file = '/content/drive/My Drive/DeepLearningforHealthcare/data/kg_grouped_diseases_bert_map.csv'\n",
        "df = pd.read_csv(file)\n",
        "\n",
        "# Display the first few rows of the DataFrame to understand its structure\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExV9tpei_HvG"
      },
      "outputs": [],
      "source": [
        "# Function to extract unique edges from the group_id_bert column\n",
        "def extract_edges(grouped_ids):\n",
        "    # Split the string by underscore and convert to set for unique ids\n",
        "    ids = set(grouped_ids.split('_'))\n",
        "    # Create a list of tuples representing edges (all possible combinations without repetition)\n",
        "    edges = [(a, b) for idx, a in enumerate(ids) for b in list(ids)[idx + 1:]]\n",
        "    return edges\n",
        "\n",
        "# Initialize an empty graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# Add nodes and edges to the graph\n",
        "for _, row in df.iterrows():\n",
        "    # Add the disease node to the graph\n",
        "    G.add_node(row['node_id'], name=row['node_name'], type=row['node_type'], source=row['node_source'])\n",
        "\n",
        "    # Add edges from this disease to others in the same group\n",
        "    edges = extract_edges(row['group_id_bert'])\n",
        "    G.add_edges_from(edges)\n",
        "\n",
        "# Now, let's visualize the graph\n",
        "plt.figure(figsize=(12, 12))\n",
        "nx.draw(G, with_labels=False, node_size=20, alpha=0.6, edge_color=\"r\", font_size=8)\n",
        "plt.title(\"Graph of Disease Connections\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3muyDPFPbozY"
      },
      "source": [
        "##   Model\n",
        "The model includes the model definitation which usually is a class, model training, and other necessary parts.\n",
        "  * Model architecture: layer number/size/type, activation function, etc\n",
        "  * Training objectives: loss function, optimizer, weight of each loss term, etc\n",
        "  * Others: whether the model is pretrained, Monte Carlo simulation for uncertainty analysis, etc\n",
        "  * The code of model should have classes of the model, functions of model training, model validation, etc.\n",
        "  * If your model training is done outside of this notebook, please upload the trained model here and develop a function to load and test it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxKB3UM5_HvH"
      },
      "outputs": [],
      "source": [
        "base_model = RNN(\n",
        "    dataset=sample_dataset,\n",
        "    feature_keys=[\"conditions\", \"procedures\"],\n",
        "    label_key=\"label\",\n",
        "    mode=\"binary\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkj54A2y_HvH"
      },
      "outputs": [],
      "source": [
        "base_trainer = Trainer(model=base_model)\n",
        "base_trainer.train(\n",
        "    train_dataloader=train_dataloader,\n",
        "    val_dataloader=val_dataloader,\n",
        "    epochs=50,\n",
        "    monitor=\"roc_auc\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVt_CmGd_HvH"
      },
      "outputs": [],
      "source": [
        "time_model = RNN(\n",
        "    dataset=sample_dataset,\n",
        "    feature_keys=[\"conditions\", \"procedures\", \"visit_diff\"],\n",
        "    label_key=\"label\",\n",
        "    mode=\"binary\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQe9jUg9_HvH"
      },
      "outputs": [],
      "source": [
        "time_trainer = Trainer(model=time_model)\n",
        "time_trainer.train(\n",
        "    train_dataloader=train_dataloader,\n",
        "    val_dataloader=val_dataloader,\n",
        "    epochs=50,\n",
        "    monitor=\"roc_auc\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBdVZoTvsSFV"
      },
      "outputs": [],
      "source": [
        "class my_model():\n",
        "  # use this class to define your model\n",
        "  pass\n",
        "\n",
        "model = my_model()\n",
        "loss_func = None\n",
        "optimizer = None\n",
        "\n",
        "def train_model_one_iter(model, loss_func, optimizer):\n",
        "  pass\n",
        "\n",
        "num_epoch = 10\n",
        "# model training loop: it is better to print the training/validation losses during the training\n",
        "for i in range(num_epoch):\n",
        "  train_model_one_iter(model, loss_func, optimizer)\n",
        "  train_loss, valid_loss = None, None\n",
        "  print(\"Train Loss: %.2f, Validation Loss: %.2f\" % (train_loss, valid_loss))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gX6bCcZNuxmz"
      },
      "source": [
        "# Results\n",
        "In this section, you should finish training your model training or loading your trained model. That is a great experiment! You should share the results with others with necessary metrics and figures.\n",
        "\n",
        "Please test and report results for all experiments that you run with:\n",
        "\n",
        "*   specific numbers (accuracy, AUC, RMSE, etc)\n",
        "*   figures (loss shrinkage, outputs from GAN, annotation or label of sample pictures, etc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjW9bCkouv8O"
      },
      "outputs": [],
      "source": [
        "# metrics to evaluate my model\n",
        "\n",
        "# plot figures to better show the results\n",
        "\n",
        "# it is better to save the numbers and figures for your presentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lB6Krn16_HvI"
      },
      "outputs": [],
      "source": [
        "# Base model performance\n",
        "\n",
        "print(base_trainer.evaluate(test_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6Nh7RPf_HvI"
      },
      "outputs": [],
      "source": [
        "# Time-enhanced model performance\n",
        "\n",
        "print(time_trainer.evaluate(test_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sejoFOah_HvI"
      },
      "outputs": [],
      "source": [
        "# Full DG-RNN model performance\n",
        "\n",
        "#print(gn_rnn_trainer.evaluate(test_dataloader))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EAWAy_LwHlV"
      },
      "source": [
        "## Model comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOdhGrbwwG71"
      },
      "outputs": [],
      "source": [
        "# compare you model with others\n",
        "# you don't need to re-run all other experiments, instead, you can directly refer the metrics/numbers in the paper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qH75TNU71eRH"
      },
      "source": [
        "# Discussion\n",
        "\n",
        "In this section,you should discuss your work and make future plan. The discussion should address the following questions:\n",
        "  * Make assessment that the paper is reproducible or not.\n",
        "  * Explain why it is not reproducible if your results are kind negative.\n",
        "  * Describe “What was easy” and “What was difficult” during the reproduction.\n",
        "  * Make suggestions to the author or other reproducers on how to improve the reproducibility.\n",
        "  * What will you do in next phase.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHMI2chl9omn"
      },
      "source": [
        "# References\n",
        "\n",
        "1.   Johnson, A., Pollard, T., & Mark, R. (2016). MIMIC-III Clinical Database (version 1.4). PhysioNet. https://doi.org/10.13026/C2XW26.\n",
        "2.   Johnson, A. E. W., Pollard, T. J., Shen, L., Lehman, L. H., Feng, M., Ghassemi, M., Moody, B., Szolovits, P., Celi, L. A., & Mark, R. G. (2016). MIMIC-III, a freely accessible critical care database. Scientific Data, 3, 160035.\n",
        "3.   Goldberger, A., Amaral, L., Glass, L., Hausdorff, J., Ivanov, P. C., Mark, R., ... & Stanley, H. E. (2000). PhysioBank, PhysioToolkit, and PhysioNet: Components of a new research resource for complex physiologic signals. Circulation [Online]. 101 (23), pp. e215–e220.\n",
        "4.   Yin, C., Zhao, R., Qian, B., Lv, X., & Zhang, P. (2019). Domain Knowledge Guided Deep Learning with Electronic Health Records. In 2019 IEEE International Conference on Data Mining (ICDM) (pp. 738-747). Beijing, China. https://doi.org/10.1109/ICDM.2019.00084\n",
        "5.   Yang, C., Wu, Z., Jiang, P., Lin, Z., Gao, J., Danek, B. P., & Sun, J. (2023). PyHealth: A Deep Learning Toolkit for Healthcare Applications. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (pp. 5788–5789). New York, NY, USA: Association for Computing Machinery.\n",
        "6.   Hochreiter, S., & Schmidhuber, J. (1997). Long Short-Term Memory. Neural Computation, 9(8), 1735-1780. https://doi.org/10.1162/neco.1997.9.8.1735\n",
        "7.   Yang, P., Wang, H., Huang, Y., Yang, S., Zhang, Y., Huang, L., Zhang, Y., Wang, G., Yang, S., He, L., & Huang, Y. (2024). LMKG: A large-scale and multi-source medical knowledge graph for intelligent medicine applications. Knowledge-Based Systems, 284, 111323. https://doi.org/10.1016/j.knosys.2023.111323"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}