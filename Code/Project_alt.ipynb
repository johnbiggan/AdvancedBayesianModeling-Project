{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johnbiggan/DeepLearningforHealthcare-Project/blob/main/Code/Project_alt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Project notebook available in Google Colab space: https://colab.research.google.com/github/johnbiggan/DeepLearningforHealthcare-Project/blob/main/Code/Project.ipynb\n",
        "\n",
        "and public GitHub repo: https://github.com/johnbiggan/DeepLearningforHealthcare-Project/blob/1da3ddf002c103638dbb566d9dffa9518eb24436/Code/Project.ipynb"
      ],
      "metadata": {
        "id": "7YYD-33pLZz-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Domain Knowledge Guided Deep Learning\n",
        "###with Electronic Health Records Replication"
      ],
      "metadata": {
        "id": "iHlDobF8_sW7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmYpM15nAQQR"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "Every year, more than a quarter of all healthcare spending in the US is spent on preventable diseases [1]. In an effort to intervene earlier researchers look to the ever-growing pool of electronic health records (EHRs) to predict disease risk. Many models have been developed, but with the recurrent nature healthcare visits recurrent neural networks (RNNs) have shown the most promise.\n",
        "\n",
        "Unfortunately, early RNNs treated the time between consecutive visits as well as the time between visits and disease diagnosis as unimportant. Additionally, other early models fail to incorporate medical knowledge in the form of relationships between diagnosis (e.g comorbid, causes, caused-by).\n",
        "\n",
        "The Domain Knowledge Guided Recurrent Neural Network (DG-RNN) being replicated in this project addresses both shortcomings and has been found to improve heart failure prediction over-and-above previous methodologies [2]. In their paper, the authors described a model that uses multiple long short-term memory models [3] with attention to utilize disease diagnoses, procedure codes, visit time differences, and a medical knowledge graph (Figure 1). Incorporating medical knowledge graphs provides challenges. For example, the semi-structured nature of graphs adds to the complexity. Moreover, readily-available medical knowledge graphs have been few and far between.\n",
        "\n",
        "Testing this model on the MIMIC-III dataset [4,5,6], it was able to outperform numerous other models, including random forests, support vector machines, and traditional long short-term memory RNNs on a heart failure prediction task.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Figure 1](https://raw.githubusercontent.com/johnbiggan/DeepLearningforHealthcare-Project/main/Figures/Figure1.png \"Figure 1\")\n",
        "\n",
        "Figure 1. DG-RNN model."
      ],
      "metadata": {
        "id": "r1VOt02kNdXv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uygL9tTPSVHB"
      },
      "source": [
        "# Scope of Reproducibility:\n",
        "\n",
        "This replication will focus on reproducing the main findings of the paper. Specifically, the following hypotheses will be tested:\n",
        "\n",
        "Hypothesis 1: Including the time between visits will improve the model over the base model that treats all visits in a simple sequential manner.\n",
        "\n",
        "Hypothesis 2: Including domain knowledge, along with the time between visits, will lead to a better performing model than the base model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4lchr9oKMh6H"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      },
      "source": [
        "# Methodology\n",
        "\n",
        "This project will use tools from the pyhealth package, which should be installed if it has not already been done."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pyhealth"
      ],
      "metadata": {
        "id": "y5Cr-eC9_TLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NbPHUTMbkD3"
      },
      "source": [
        "##  Data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MIMIC-III\n",
        "\n",
        "For their model, the authors used a proprietary dataset as well as the MIMIC-III (Medical Information Mart for Intensive Care III) dataset [4,5,6], which is a freely available database for researchers consisting of deidentified health data from over 40,000 patients who stayed in critical care units of the Beth Israel Deaconess Medical Center between 2001 and 2012. It includes detailed demographics and visit-level healthcare data.\n",
        "\n",
        "Although the data is freely available to researchers, it is not publically available due to privacy concerns. Fortunately, in recent years pyhealth [7] contributors have created a synthetic dataset based on the MIMIC-III data that does not create the same privacy concerns. As this is a public-facing project, the synthetic MIMIC-III data from pyhealth will be used. However, the same analyses may be conducted with the original MIMIC-III dataset by changing the **root** argument to point at the original dataset."
      ],
      "metadata": {
        "id": "n2jgcV6fOz6w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzVUQS0CHry0"
      },
      "outputs": [],
      "source": [
        "from pyhealth.datasets import MIMIC3Dataset\n",
        "from pyhealth.datasets import split_by_patient, get_dataloader\n",
        "from pyhealth.models import RNN\n",
        "from pyhealth.models import RETAIN\n",
        "from pyhealth.trainer import Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aYF3qCv_HvF"
      },
      "outputs": [],
      "source": [
        "base_dataset = MIMIC3Dataset(\n",
        "    root=\"https://storage.googleapis.com/pyhealth/Synthetic_MIMIC-III/\",\n",
        "    tables=[\"DIAGNOSES_ICD\", \"PROCEDURES_ICD\"],\n",
        "    code_mapping={\"ICD9CM\": \"CCSCM\", \"ICD9PROC\": \"CCSPROC\"},\n",
        "    dev=False,\n",
        "    refresh_cache=False,\n",
        ")\n",
        "base_dataset.stat()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsLJXUBQ_HvF"
      },
      "outputs": [],
      "source": [
        "# Confirm the CCSCM code for conjestive heart failure to be used in the labeling task\n",
        "from pyhealth.medcode import CrossMap\n",
        "from pyhealth.medcode import InnerMap\n",
        "\n",
        "mapping = CrossMap.load(source_vocabulary=\"ICD9CM\", target_vocabulary=\"CCSCM\")\n",
        "print(\"The CCSCM code that maps to ICD-9 code 428.0 (conjestive heart failure) is\", mapping.map(\"428.0\"))\n",
        "print(\"The CCSCM code that maps to ICD-9 code 428.9 (conjestive heart failure) is\", mapping.map(\"428.9\"))\n",
        "\n",
        "ccscm = InnerMap.load(\"CCSCM\")\n",
        "print(\"Confirmation that CCSCM code '108' corresponds to\", ccscm.lookup(\"108\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWOyusf9_HvF"
      },
      "outputs": [],
      "source": [
        "# Create custom visit time difference calculation and heart failure prediction task\n",
        "from pyhealth.data import Patient, Visit\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def visit_time_diff_mimic3_fn(patient: Patient):\n",
        "    \"\"\"Processes a single patient for the visit time difference task.\n",
        "\n",
        "    Visit time difference calculates the delay between the current visit and\n",
        "    the previous visit.\n",
        "\n",
        "    Args:\n",
        "        patient: a Patient object\n",
        "\n",
        "    Returns:\n",
        "        samples: a list of samples, each sample is a dict with patient_id,\n",
        "            visit_id, and other task-specific attributes as key\n",
        "\n",
        "    Examples:\n",
        "        >>> from pyhealth.datasets import MIMIC3Dataset\n",
        "        >>> mimic3_base = MIMIC3Dataset(\n",
        "        ...    root=\"/srv/local/data/physionet.org/files/mimiciii/1.4\",\n",
        "        ...    tables=[\"DIAGNOSES_ICD\", \"PROCEDURES_ICD\", \"PRESCRIPTIONS\"],\n",
        "        ...    code_mapping={\"ICD9CM\": \"CCSCM\"},\n",
        "        ... )\n",
        "        >>> from pyhealth.tasks import hf_prediction_mimic3_fn\n",
        "        >>> mimic3_sample = mimic3_base.set_task(visit_time_diff_mimic3_fn)\n",
        "        >>> mimic3_sample.samples[0]\n",
        "        [{'visit_id': '130744', 'patient_id': '103', 'conditions': [['42', '109', '19', '122', '98', '663', '58', '51']], 'procedures': [['1']], 'visit_diff': [[0.0, 0.0, 0.0, 0.0]] 'label': 0}]\n",
        "    \"\"\"\n",
        "    samples = []\n",
        "    criterion_time = None\n",
        "\n",
        "    for i in range(len(patient) - 1):\n",
        "        visit: Visit = patient[i]\n",
        "        visit_time = visit.encounter_time\n",
        "\n",
        "        next_visit: Visit = patient[i + 1]\n",
        "        hf_label = 0\n",
        "\n",
        "        if '108' not in next_visit.get_code_list(table=\"DIAGNOSES_ICD\"):\n",
        "            hf_label = 0\n",
        "        else:\n",
        "            hf_label = 1\n",
        "\n",
        "        visit_diff = []\n",
        "\n",
        "        if i > 0:\n",
        "            prev_visit: Visit = patient[i - 1]\n",
        "            prev_visit_time = prev_visit.encounter_time\n",
        "\n",
        "            # Find criterion time (i.e. first encounter with HF diagnosis)\n",
        "            for j in range(len(patient) - 1):\n",
        "                c_visit: Visit = patient[j]\n",
        "                if criterion_time == None and '108' in c_visit.get_code_list(table=\"DIAGNOSES_ICD\"):\n",
        "                    criterion_time = c_visit.encounter_time\n",
        "\n",
        "            if criterion_time != None:\n",
        "                v_c_s = np.sin(((visit_time - criterion_time).days)/10000).item()\n",
        "                v_c_c = np.cos(((visit_time - criterion_time).days)/10000).item()\n",
        "            else:\n",
        "                v_c = 0.0\n",
        "                v_c_s = 0.0\n",
        "                v_c_c = 0.0\n",
        "\n",
        "            v_p = visit_time - prev_visit_time\n",
        "\n",
        "            visit_diff = [\n",
        "                v_c_s,\n",
        "                v_c_c,\n",
        "                np.sin(((v_p).days)/10000).item(),\n",
        "                np.cos(((v_p).days)/10000).item()\n",
        "            ]\n",
        "\n",
        "        else:\n",
        "            visit_diff = [0.0, 0.0, 0.0, 0.0]\n",
        "\n",
        "        conditions = visit.get_code_list(table=\"DIAGNOSES_ICD\")\n",
        "        procedures = visit.get_code_list(table=\"PROCEDURES_ICD\")\n",
        "        # exclude: visits without condition and procedure code\n",
        "        if len(conditions) * len(procedures) == 0 or len(patient) < 2:\n",
        "            continue\n",
        "        samples.append(\n",
        "            {\n",
        "                \"visit_id\": visit.visit_id,\n",
        "                \"patient_id\": patient.patient_id,\n",
        "                \"conditions\": [conditions],\n",
        "                \"procedures\": [procedures],\n",
        "                \"visit_diff\": [visit_diff],\n",
        "                \"label\": hf_label,\n",
        "            }\n",
        "        )\n",
        "    # no cohort selection\n",
        "    return samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFsqr4s6_HvG"
      },
      "outputs": [],
      "source": [
        "# Create a sample dataset with a label to indicate an HF diagnosis and visit difference vector\n",
        "sample_dataset = base_dataset.set_task(visit_time_diff_mimic3_fn)\n",
        "sample_dataset.stat()\n",
        "\n",
        "# Split dataset into training, validation, and testing using an 80%, 10%, 10% split\n",
        "train_dataset, val_dataset, test_dataset = split_by_patient(\n",
        "    sample_dataset, [0.8, 0.1, 0.1]\n",
        ")\n",
        "\n",
        "train_dataloader = get_dataloader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_dataloader = get_dataloader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_dataloader = get_dataloader(test_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# View a single patient\n",
        "print(sample_dataset[9]) #9"
      ],
      "metadata": {
        "id": "uGadm3-3bkFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PrimeKG\n",
        "\n",
        "For the knowledge graph, the authors originally used the KnowLife knowledge graph [8]. Unfortunately, at the time of this replication the KnowLife knowledge graph was no longer available.\n",
        "\n",
        "More recently, researchers have developed an updated medical knowledge graph called PrimeKG [9,10]. This appears to be a worthwhile alternative. At the time of this writing, this is being explored. One challenge is that the nodes are coded in Monarch Disease Ontology (MONDO) coding, for which there is not a simple mapping available."
      ],
      "metadata": {
        "id": "32vMvDdURIMr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiHVzIcG_HvG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import io\n",
        "\n",
        "# Load the CSV data into a pandas DataFrame\n",
        "url = \"https://raw.githubusercontent.com/johnbiggan/DeepLearningforHealthcare-Project/main/Data/kg_grouped_diseases_bert_map.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Display the first few rows of the DataFrame to understand its structure\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExV9tpei_HvG"
      },
      "outputs": [],
      "source": [
        "# Function to extract unique edges from the group_id_bert column\n",
        "def extract_edges(grouped_ids):\n",
        "    # Split the string by underscore and convert to set for unique ids\n",
        "    ids = set(grouped_ids.split('_'))\n",
        "    # Create a list of tuples representing edges (all possible combinations without repetition)\n",
        "    edges = [(a, b) for idx, a in enumerate(ids) for b in list(ids)[idx + 1:]]\n",
        "    return edges\n",
        "\n",
        "# Initialize an empty graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# Add nodes and edges to the graph\n",
        "for _, row in df.iterrows():\n",
        "    # Add the disease node to the graph\n",
        "    G.add_node(row['node_id'], name=row['node_name'], type=row['node_type'], source=row['node_source'])\n",
        "\n",
        "    # Add edges from this disease to others in the same group\n",
        "    edges = extract_edges(row['group_id_bert'])\n",
        "    G.add_edges_from(edges)\n",
        "\n",
        "# Visualize the graph\n",
        "plt.figure(figsize=(12, 12))\n",
        "nx.draw(G, with_labels=False, node_size=20, alpha=0.6, edge_color=\"r\", font_size=8)\n",
        "plt.title(\"Graph of Disease Connections\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3muyDPFPbozY"
      },
      "source": [
        "##   Model\n",
        "The model includes the model definitation which usually is a class, model training, and other necessary parts.\n",
        "  * Model architecture: layer number/size/type, activation function, etc\n",
        "  * Training objectives: loss function, optimizer, weight of each loss term, etc\n",
        "  * Others: whether the model is pretrained, Monte Carlo simulation for uncertainty analysis, etc\n",
        "  * The code of model should have classes of the model, functions of model training, model validation, etc.\n",
        "  * If your model training is done outside of this notebook, please upload the trained model here and develop a function to load and test it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxKB3UM5_HvH"
      },
      "outputs": [],
      "source": [
        "base_model = RNN(\n",
        "    dataset=sample_dataset,\n",
        "    feature_keys=[\"conditions\", \"procedures\"],\n",
        "    label_key=\"label\",\n",
        "    mode=\"binary\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkj54A2y_HvH"
      },
      "outputs": [],
      "source": [
        "base_trainer = Trainer(model=base_model)\n",
        "base_trainer.train(\n",
        "    train_dataloader=train_dataloader,\n",
        "    val_dataloader=val_dataloader,\n",
        "    epochs=50,\n",
        "    monitor=\"roc_auc\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVt_CmGd_HvH"
      },
      "outputs": [],
      "source": [
        "time_model = RNN(\n",
        "    dataset=sample_dataset,\n",
        "    feature_keys=[\"conditions\", \"procedures\", \"visit_diff\"],\n",
        "    label_key=\"label\",\n",
        "    mode=\"binary\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQe9jUg9_HvH"
      },
      "outputs": [],
      "source": [
        "time_trainer = Trainer(model=time_model)\n",
        "time_trainer.train(\n",
        "    train_dataloader=train_dataloader,\n",
        "    val_dataloader=val_dataloader,\n",
        "    epochs=50,\n",
        "    monitor=\"roc_auc\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retain_model = RETAIN(\n",
        "    dataset=sample_dataset,\n",
        "    feature_keys=[\"conditions\", \"procedures\"],\n",
        "    label_key=\"label\",\n",
        "    mode=\"binary\",\n",
        ")"
      ],
      "metadata": {
        "id": "KO0quf-C5O-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retain_trainer = Trainer(model=retain_model)\n",
        "retain_trainer.train(\n",
        "    train_dataloader=train_dataloader,\n",
        "    val_dataloader=val_dataloader,\n",
        "    epochs=50,\n",
        "    monitor=\"roc_auc\",\n",
        ")"
      ],
      "metadata": {
        "id": "CH3AjbOH5Q8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBdVZoTvsSFV"
      },
      "outputs": [],
      "source": [
        "class my_model():\n",
        "  # use this class to define your model\n",
        "  pass\n",
        "\n",
        "model = my_model()\n",
        "loss_func = None\n",
        "optimizer = None\n",
        "\n",
        "def train_model_one_iter(model, loss_func, optimizer):\n",
        "  pass\n",
        "\n",
        "num_epoch = 10\n",
        "# model training loop: it is better to print the training/validation losses during the training\n",
        "for i in range(num_epoch):\n",
        "  train_model_one_iter(model, loss_func, optimizer)\n",
        "  train_loss, valid_loss = None, None\n",
        "  print(\"Train Loss: %.2f, Validation Loss: %.2f\" % (train_loss, valid_loss))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gX6bCcZNuxmz"
      },
      "source": [
        "# Results\n",
        "In this section, you should finish training your model training or loading your trained model. That is a great experiment! You should share the results with others with necessary metrics and figures.\n",
        "\n",
        "Please test and report results for all experiments that you run with:\n",
        "\n",
        "*   specific numbers (accuracy, AUC, RMSE, etc)\n",
        "*   figures (loss shrinkage, outputs from GAN, annotation or label of sample pictures, etc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjW9bCkouv8O"
      },
      "outputs": [],
      "source": [
        "# metrics to evaluate my model\n",
        "\n",
        "# plot figures to better show the results\n",
        "\n",
        "# it is better to save the numbers and figures for your presentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lB6Krn16_HvI"
      },
      "outputs": [],
      "source": [
        "# Base model performance\n",
        "\n",
        "print(base_trainer.evaluate(test_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6Nh7RPf_HvI"
      },
      "outputs": [],
      "source": [
        "# Time-enhanced model performance\n",
        "\n",
        "print(time_trainer.evaluate(test_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RETAIN model performance\n",
        "\n",
        "print(retain_trainer.evaluate(test_dataloader))"
      ],
      "metadata": {
        "id": "RvW-WMj95z8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sejoFOah_HvI"
      },
      "outputs": [],
      "source": [
        "# Full DG-RNN model performance\n",
        "\n",
        "#print(gn_rnn_trainer.evaluate(test_dataloader))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EAWAy_LwHlV"
      },
      "source": [
        "## Model comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOdhGrbwwG71"
      },
      "outputs": [],
      "source": [
        "# compare you model with others\n",
        "# you don't need to re-run all other experiments, instead, you can directly refer the metrics/numbers in the paper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qH75TNU71eRH"
      },
      "source": [
        "# Discussion\n",
        "\n",
        "In this section,you should discuss your work and make future plan. The discussion should address the following questions:\n",
        "  * Make assessment that the paper is reproducible or not.\n",
        "  * Explain why it is not reproducible if your results are kind negative.\n",
        "  * Describe “What was easy” and “What was difficult” during the reproduction.\n",
        "  * Make suggestions to the author or other reproducers on how to improve the reproducibility.\n",
        "  * What will you do in next phase.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHMI2chl9omn"
      },
      "source": [
        "# References\n",
        "\n",
        "1.   Bolnick, H. J., Bui, A. L., Bulchis, A., Chen, C., Chapin, A., Lomsadze, L., ... Dieleman, J. L. (2020). Health-care spending attributable to modifiable risk factors in the USA: An economic attribution analysis. The Lancet Public Health, 5(10), e525-e535. https://doi.org/10.1016/S2468-2667(20)30203-6\n",
        "2.   Yin, C., Zhao, R., Qian, B., Lv, X., & Zhang, P. (2019). Domain Knowledge Guided Deep Learning with Electronic Health Records. In 2019 IEEE International Conference on Data Mining (ICDM) (pp. 738-747). Beijing, China. https://doi.org/10.1109/ICDM.2019.00084\n",
        "3.   Hochreiter, S., & Schmidhuber, J. (1997). Long Short-Term Memory. Neural Computation, 9(8), 1735-1780. https://doi.org/10.1162/neco.1997.9.8.1735\n",
        "4.   Johnson, A., Pollard, T., & Mark, R. (2016). MIMIC-III Clinical Database (version 1.4). PhysioNet. https://doi.org/10.13026/C2XW26.\n",
        "5.   Johnson, A. E. W., Pollard, T. J., Shen, L., Lehman, L. H., Feng, M., Ghassemi, M., Moody, B., Szolovits, P., Celi, L. A., & Mark, R. G. (2016). MIMIC-III, a freely accessible critical care database. Scientific Data, 3, 160035.\n",
        "6.   Goldberger, A., Amaral, L., Glass, L., Hausdorff, J., Ivanov, P. C., Mark, R., ... & Stanley, H. E. (2000). PhysioBank, PhysioToolkit, and PhysioNet: Components of a new research resource for complex physiologic signals. Circulation [Online]. 101 (23), pp. e215–e220.\n",
        "7.   Yang, C., Wu, Z., Jiang, P., Lin, Z., Gao, J., Danek, B. P., & Sun, J. (2023). PyHealth: A Deep Learning Toolkit for Healthcare Applications. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (pp. 5788–5789). New York, NY, USA: Association for Computing Machinery.\n",
        "8.   Ernst, P., Siu, A., & Weikum, G. (2015). Knowlife: A versatile approach for constructing a large knowledge graph for biomedical sciences. BMC Bioinformatics.\n",
        "9.   Chandak, P., Huang, K., & Zitnik, M. (2023). Building a knowledge graph to enable precision medicine. Scientific Data, 10(1), Article 67. https://doi.org/10.1038/s41597-023-01960-3\n",
        "10.   Chandak, P. (2022). PrimeKG (V2) [Data set]. Harvard Dataverse. https://doi.org/10.7910/DVN/IXA7BM"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}